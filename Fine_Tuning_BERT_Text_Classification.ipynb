{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Text Classification with Fine Tuning BERT**\n",
        "### **Apa itu BERT?**\n",
        "BERT adalah singkatan dari **B**idirectional **E**ncoder **R**epresentations from **T**ransformers. Dari kepanjangannya ada dua hal yang dapat digaris bawahi.\n",
        "BERT menggunakan salah satu bagian dari transformer, yaitu encoder.\n",
        "BERT itu bidirectional, artinya dilatih dari dua arah (kiri-kanan dan kanan-kiri).\n",
        "\n",
        "Ada dua model BERT yang berbeda:\n",
        "\n",
        "1. BERT Base , yang merupakan model BERT terdiri dari 12 Transformer encoder layers, 12 attention heads, 768 hidden size, dan 110M parameters.\n",
        "2. BERT Large , yang merupakan model BERT terdiri dari 24 Transformer encoder layers, 16 attention heads, 1024 hidden size, dan 340 parameters.\n",
        "\n",
        "### **Keuntungan dari Fine Tuning**\n",
        "Kita akan menggunakan BERT untuk melatih pengklasifikasi teks. Secara khusus, kita akan mengambil model BERT yang telah dilatih sebelumnya, dan melatih model baru untuk tugas klasifikasi kami. Mengapa melakukan ini daripada melatih model pembelajaran mendalam tertentu (CNN, BiLSTM, dll.) yang sangat cocok untuk tugas NLP spesifik yang kita butuhkan?\n",
        "\n",
        "1. **Pengembangan Lebih Cepat**\n",
        "\n",
        "    * Pertama, bobot model BERT yang telah dilatih sebelumnya telah mengkodekan banyak informasi tentang bahasa kita. Akibatnya, dibutuhkan lebih sedikit waktu untuk melatih model fine-tuned kami - seolah-olah kami telah melatih lapisan bawah jaringan kami secara ekstensif dan hanya perlu menyetelnya dengan lembut saat menggunakan outputnya sebagai fitur untuk tugas klasifikasi kami. Faktanya, penulis merekomendasikan hanya 2-4 periode pelatihan untuk menyempurnakan BERT pada tugas NLP tertentu (dibandingkan dengan ratusan jam GPU yang diperlukan untuk melatih model BERT asli atau LSTM dari awal!).\n",
        "\n",
        "2. **Lebih Sedikit Data**\n",
        "\n",
        "    * Selain itu dan mungkin sama pentingnya, karena bobot yang telah dilatih sebelumnya, metode ini memungkinkan kita untuk menyempurnakan tugas kita pada kumpulan data yang jauh lebih kecil daripada yang diperlukan dalam model yang dibangun dari awal. Kelemahan utama dari model NLP yang dibangun dari awal adalah bahwa kita sering membutuhkan kumpulan data yang sangat besar untuk melatih jaringan kita ke akurasi yang wajar, yang berarti banyak waktu dan energi harus dimasukkan ke dalam pembuatan kumpulan data. Dengan menyempurnakan BERT, kami sekarang dapat melatih model untuk kinerja yang baik pada jumlah data pelatihan yang jauh lebih kecil.\n",
        "\n",
        "3. **Hasil Lebih Baik**\n",
        "\n",
        "    * Akhirnya, prosedur fine-tuning sederhana ini (biasanya menambahkan satu lapisan yang terhubung penuh di atas BERT dan pelatihan untuk beberapa zaman) ditunjukkan untuk mencapai hasil seni dengan penyesuaian khusus tugas minimal untuk berbagai tugas: klasifikasi, inferensi bahasa, kesamaan semantik, penjawab pertanyaan, dll. Daripada menerapkan arsitektur khusus dan kadang-kadang-kabur yang terbukti bekerja dengan baik pada tugas tertentu, hanya menyempurnakan BERT terbukti menjadi alternatif yang lebih baik (atau setidaknya sama).\n",
        "\n"
      ],
      "metadata": {
        "id": "91DKU-Mx1g7K"
      },
      "id": "91DKU-Mx1g7K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Gunakan GPU untuk melatih model karena model dasar BERT berisi 110 juta parameter.***\n",
        "\n",
        "`Edit ðŸ¡’ Notebook Settings ðŸ¡’ Hardware accelerator ðŸ¡’ (GPU)`"
      ],
      "metadata": {
        "id": "X2R6B0by5Quz"
      },
      "id": "X2R6B0by5Quz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup library and utility**"
      ],
      "metadata": {
        "id": "Aq_hucQF46tK"
      },
      "id": "Aq_hucQF46tK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Installing Hugging Face Library**\n",
        "\n",
        "Instal paket [transformers](https://github.com/huggingface/transformers) dari Hugging Face yang akan memberi kita antarmuka pytorch untuk bekerja dengan BERT. (Library ini berisi antarmuka untuk model bahasa pra-latihan lainnya seperti OpenAI's GPT dan GPT-2.) Kami telah memilih antarmuka pytorch karena memberikan keseimbangan yang baik antara API tingkat tinggi (yang mudah digunakan tetapi tidak memberikan wawasan ke dalam cara kerja sesuatu).\n",
        "\n",
        "Saat ini, library Hugging Face tampaknya menjadi interface untuk pytorch yang paling banyak diterima dan kuat untuk bekerja dengan BERT. Selain mendukung berbagai model transformers pre-trained yang berbeda, perpustakaan juga menyertakan modifikasi pre-built dari model ini yang sesuai dengan tugas spesifik Anda. Misalnya, dalam tugas ini kita akan menggunakan `BertForSequenceClassification`.\n",
        "\n",
        "Library juga menyertakan task-specific classes untuk klasifikasi token, question answering, next sentence prediciton, dll. Menggunakan kelas yang dibuat sebelumnya ini menyederhanakan proses modifikasi BERT untuk tujuan Anda."
      ],
      "metadata": {
        "id": "Ssl2vTm-6GKy"
      },
      "id": "Ssl2vTm-6GKy"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "NEuIQ5d5w5xd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEuIQ5d5w5xd",
        "outputId": "53bf68db-dbae-4c50-b6c7-16215adccc86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cd4dce5e",
      "metadata": {
        "id": "cd4dce5e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, get_linear_schedule_with_warmup\n",
        "\n",
        "from data_utils import TextClassificationDataset, TextClassificationDataLoader\n",
        "from metrics import text_classification_metrics_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f40ee2cb",
      "metadata": {
        "id": "f40ee2cb"
      },
      "source": [
        "### **Tokenization & Input Formatting**\n",
        "\n",
        "Kita akan mengubah dataset kami ke dalam format yang dapat dilatih BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **BERT Tokenizer**\n",
        "\n",
        "Untuk memasukkan teks kita ke BERT, itu harus dipecah menjadi token, dan kemudian token ini harus dipetakan ke indeksnya dalam kosakata tokenizer.\n",
        "\n",
        "Tokenisasi harus dilakukan oleh tokenizer yang disertakan dengan BERT--sel di bawah ini akan mengunduhnya. Kita akan menggunakan versi \"indobenchmark/indobert-base-p1\" di sini."
      ],
      "metadata": {
        "id": "OP88T4sX7o89"
      },
      "id": "OP88T4sX7o89"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "62fc19cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62fc19cd",
        "outputId": "0fab02fc-54d8-4f20-d5e9-ad4e6086b51b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Load Tokenizer and Config\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1', do_lower_case=True)\n",
        "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
        "config.num_labels = TextClassificationDataset.NUM_LABELS\n",
        "\n",
        "# Instantiate model\n",
        "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the original sentence.\n",
        "print(' Original: ', \"Aku adalah bocah petualang\")\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(\"Aku adalah bocah petualang\"))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"Aku adalah bocah petualang\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gWqCdNB8fq1",
        "outputId": "7d10926e-95da-4d23-84d7-952b477f7581"
      },
      "id": "0gWqCdNB8fq1",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Original:  Aku adalah bocah petualang\n",
            "Tokenized:  ['aku', 'adalah', 'bocah', 'petualang']\n",
            "Token IDs:  [304, 154, 10829, 27606]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ebc9a9f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebc9a9f4",
        "outputId": "94c00a7f-e3ca-4ef4-a6c0-7d9ff8c50725"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all of the model's parameters as a list of tuples.\n",
        "params = list(model.named_parameters())\n",
        "\n",
        "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "print('==== Embedding Layer ====\\n')\n",
        "\n",
        "for p in params[0:5]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "for p in params[5:21]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "for p in params[-4:]:\n",
        "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_PJNbtQQ94f",
        "outputId": "7c44cba0-b8d9-4969-f07b-78b81d0539a2"
      },
      "id": "K_PJNbtQQ94f",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 201 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (50000, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "bert.pooler.dense.weight                                  (768, 768)\n",
            "bert.pooler.dense.bias                                        (768,)\n",
            "classifier.weight                                           (2, 768)\n",
            "classifier.bias                                                 (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc412d4",
      "metadata": {
        "id": "5fc412d4"
      },
      "source": [
        "**Prepare Dataset & Required Formatting**\n",
        "\n",
        "*Catatan : Format input ke BERT tampaknya \"terlalu ditentukan\", kita diminta untuk memberikan sejumlah informasi yang tampaknya berlebihan, atau seperti mereka dapat dengan mudah disimpulkan dari data tanpa kami berikan secara eksplisit dia. Tapi memang begitu, dan kita kira itu akan lebih masuk akal setelah kita memiliki pemahaman yang lebih dalam tentang internal BERT.*\n",
        "\n",
        "Kita diharuskan untuk:\n",
        "1. Tambahkan ***Add special tokens*** ke awal dan akhir setiap kalimat.\n",
        "2. ***Pad & truncate*** semua kalimat menjadi satu panjang konstan.\n",
        "3. Secara eksplisit membedakan token asli dari token padding dengan \"***attention mask***\".\n",
        "\n",
        "### **Special Tokens**\n",
        "\n",
        "**`[SEP]`**\n",
        "\n",
        "Di akhir setiap kalimat, kita perlu menambahkan token `[SEP]` khusus.\n",
        "\n",
        "Token ini adalah artefak two-sentence tasks, di mana BERT diberikan dua kalimat terpisah dan diminta untuk menentukan sesuatu (misalnya, dapatkah jawaban pertanyaan di kalimat A ditemukan di kalimat B?).\n",
        "\n",
        "**`[CLS]`**\n",
        "\n",
        "Untuk tugas klasifikasi, kita harus menambahkan token `[CLS]` khusus di awal setiap kalimat.\n",
        "\n",
        "Token ini memiliki arti khusus. BERT terdiri dari 12 lapisan Transformer. Setiap transformator mengambil daftar penyematan token, dan menghasilkan jumlah penyematan yang sama pada output (tetapi dengan nilai fitur yang berubah).\n",
        "\n",
        "![Ilustrasi tujuan token CLS](https://drive.google.com/uc?export=view&id=1ck4mvGkznVJfW3hv6GUqcdGepVTOx7HE)\n",
        "\n",
        "Pada output transformator terakhir (12), *hanya embedding pertama (sesuai dengan token [CLS]) yang digunakan oleh classifier*.\n",
        "\n",
        "> \"Token pertama dari setiap urutan selalu merupakan token klasifikasi khusus (`[CLS]`). Status tersembunyi terakhir\n",
        "sesuai dengan token ini digunakan sebagai representasi urutan agregat untuk klasifikasi\n",
        "tugas.\" (dari [kertas BERT](https://arxiv.org/pdf/1810.04805.pdf))\n",
        "\n",
        "### **Sentence Length & Attention Mask**\n",
        "\n",
        "Kalimat dalam dataset kita jelas memiliki panjang yang bervariasi, jadi bagaimana BERT menangani ini?\n",
        "\n",
        "BERT memiliki dua kendala:\n",
        "1. Semua kalimat harus diisi atau dipotong menjadi satu panjang yang tetap.\n",
        "2. Panjang kalimat maksimum adalah 512 token.\n",
        "\n",
        "Padding dilakukan dengan token `[PAD]` khusus, yang berada pada indeks 0 dalam kosakata BERT. Ilustrasi di bawah ini menunjukkan padding ke \"MAX_LEN\" dari 8 token.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cb5xeqLu_5vPOgs3eRnail2Y00Fl2pCo\" width=\"600\">\n",
        "\n",
        "\"Attention Mask\" merupakan sebuah array dari 1 dan 0 yang menunjukkan token mana yang diisi dan mana yang tidak. Mask ini memberi tahu mekanisme \"Self-Attention\" di BERT untuk tidak memasukkan token PAD ini ke dalam interpretasi kalimatnya."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6c70419e",
      "metadata": {
        "id": "6c70419e"
      },
      "outputs": [],
      "source": [
        "train_dataset_path = \"dataset/data_worthcheck/train.csv\"\n",
        "dev_dataset_path = \"dataset/data_worthcheck/dev.csv\"\n",
        "test_dataset_path = \"dataset/data_worthcheck/test.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenize Dataset**\n",
        "\n",
        "Library transformers menyediakan fungsi `encode` yang membantu yang akan menangani sebagian besar langkah-langkah penguraian dan persiapan data untuk kita.\n",
        "\n",
        "Pada fungsi `TextClassificationDataset` dan `TextClassificationDataLoader` berisikan fungsi-fungsi dan metode untuk melakukan encoding, menambahkan special tokens, memberikan padding, dan menentukan mana yang termasuk attention mask."
      ],
      "metadata": {
        "id": "OM7Ymw2xBsLn"
      },
      "id": "OM7Ymw2xBsLn"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f1f82292",
      "metadata": {
        "id": "f1f82292"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextClassificationDataset(train_dataset_path, tokenizer)\n",
        "dev_dataset = TextClassificationDataset(dev_dataset_path, tokenizer)\n",
        "test_dataset = TextClassificationDataset(test_dataset_path, tokenizer)\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here. For fine-tuning BERT on a specific task, recommended a batch are size of 16 or 32.\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order. \n",
        "train_loader = TextClassificationDataLoader(train_dataset, max_len=512, batch_size=16, shuffle=True)\n",
        "dev_loader = TextClassificationDataLoader(dev_dataset, max_len=512, batch_size=16, shuffle=False)\n",
        "test_loader = TextClassificationDataLoader(test_dataset, max_len=512, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "852c7ab0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "852c7ab0",
        "outputId": "3e34fc19-afdf-4449-e82f-abbfc81192e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'no': 0, 'yes': 1}\n",
            "{0: 'no', 1: 'yes'}\n"
          ]
        }
      ],
      "source": [
        "w2i, i2w = TextClassificationDataset.LABEL2INDEX, TextClassificationDataset.INDEX2LABEL\n",
        "print(w2i)\n",
        "print(i2w)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf515dca",
      "metadata": {
        "id": "cf515dca"
      },
      "source": [
        "### **TESTING MODEL ON SENTENCE IN DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "d6398eda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6398eda",
        "outputId": "c9f9fd24-ad8d-4f51-8c80-29a3a4b42ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: neng solo wes ono terduga corona cobo neng ati mu neng conora | Label: no (51.66%)\n"
          ]
        }
      ],
      "source": [
        "text = train_dataset.__getitem__(3)[2] \n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label: {i2w[label]} ({F.softmax(logits, dim=1).squeeze()[label]*100:.2f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Optimizer & Learning Rate Scheduler**\n",
        "\n",
        "Untuk tujuan penyempurnaan, penulis merekomendasikan untuk memilih dari nilai-nilai berikut (dari Lampiran A.3 dari [kertas BERT](https://arxiv.org/pdf/1810.04805.pdf)):\n",
        "\n",
        ">- **Ukuran batch:** 16, 32\n",
        "- **Kecepatan pembelajaran (Adam):** 5e-5, 3e-5, 2e-5\n",
        "- **Jumlah epoch:** 2, 3, 4\n",
        "\n",
        "Kami memilih:\n",
        "* Ukuran batch: 16 (diatur saat membuat DataLoaders)\n",
        "* Tingkat pembelajaran: 2e-5\n",
        "* Epochs: 2"
      ],
      "metadata": {
        "id": "G-h1vmDIDmOk"
      },
      "id": "G-h1vmDIDmOk"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "607c2869",
      "metadata": {
        "id": "607c2869"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
        "model = model.cuda()\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "# training data.\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 24092022\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Loop**\n",
        "\n",
        "**Pelatihan:**\n",
        "- Unpack input dan label data\n",
        "- Load data ke GPU untuk akselerasi\n",
        "- Hapus gradien yang dihitung di lintasan sebelumnya.\n",
        "     - Di pytorch, gradien terakumulasi secara default (berguna untuk hal-hal seperti RNN) kecuali Anda menghapusnya secara eksplisit.\n",
        "- Forward pass\n",
        "- Backward pass\n",
        "- Beri tahu jaringan untuk memperbarui parameter dengan optimizer.step()\n",
        "- Lacak variabel untuk memantau kemajuan progress\n",
        "\n",
        "**Evaluasi:**\n",
        "- Unpack input dan label data\n",
        "- Load data ke GPU untuk akselerasi\n",
        "- Forward pass\n",
        "- Hitung kerugian pada data validasi kami dan lacak variabel untuk memantau kemajuan progress"
      ],
      "metadata": {
        "id": "kSaj2O9EEB37"
      },
      "id": "kSaj2O9EEB37"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8190c52e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8190c52e",
        "outputId": "56a62021-047a-4f59-b150-62c24404fafd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1351/1351 [05:00<00:00,  4.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.278 | Train Acc: 93.80% | Train F1: 92.61% | Train Precision: 91.36% | Train Recall: 94.22%\n",
            "Epoch 2/2\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1351/1351 [04:57<00:00,  4.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.132 | Train Acc: 97.79% | Train F1: 97.28% | Train Precision: 97.12% | Train Recall: 97.44%\n"
          ]
        }
      ],
      "source": [
        "#TRAINING MODEL\n",
        "\n",
        "# ========================================\n",
        "#               Training\n",
        "# ========================================\n",
        "def train(model, train_loader, dev_loader, optimizer, device, epochs=2):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    best_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch+1}/{epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        train_loss = 0\n",
        "        train_acc = 0\n",
        "        train_steps = 0\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for batch in tqdm(train_loader):\n",
        "            # Always clear any previously calculated gradients before performing a\n",
        "            # backward pass. PyTorch doesn't do this automatically because \n",
        "            # accumulating the gradients is \"convenient while training RNNs\". \n",
        "            optimizer.zero_grad()\n",
        "            loss, _, _ = forward_sequence_classification(model, batch[:-1], i2w=i2w, device=\"cuda\")\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate.\n",
        "            scheduler.step()\n",
        "\n",
        "            # Accumulate the training loss over all of the batches so that we can\n",
        "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "            # single value; the `.item()` function just returns the Python value \n",
        "            # from the tensor.\n",
        "            train_loss += loss.item()\n",
        "            train_steps += 1\n",
        "\n",
        "        train_loss /= train_steps\n",
        "        train_metrics = evaluate(model, train_loader, device)\n",
        "        dev_metrics = evaluate(model, dev_loader, device)\n",
        "        train_acc = train_metrics['accuracy']\n",
        "        train_f1 = train_metrics['f1']\n",
        "        train_precision = train_metrics['precision']\n",
        "        train_recall = train_metrics['recall']\n",
        "        print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Train F1: {train_f1*100:.2f}% | Train Precision: {train_precision*100:.2f}% | Train Recall: {train_recall*100:.2f}%')\n",
        "      \n",
        "def evaluate(model, data_loader, device):\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Tell pytorch not to bother with constructing the compute graph during\n",
        "    # the forward pass, since this is only needed for backprop (training).\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            _, y_true_batch, y_pred_batch = forward_sequence_classification(model, batch[:-1], i2w=i2w, device=\"cuda\")\n",
        "            y_true.extend(y_true_batch)\n",
        "            y_pred.extend(y_pred_batch)\n",
        "    # model.train()\n",
        "    return text_classification_metrics_fn(y_pred, y_true)\n",
        "\n",
        "def forward_sequence_classification(model, batch, i2w, device):\n",
        "    # Unpack this training batch from our dataloader. \n",
        "    #\n",
        "    # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "    # `to` method.\n",
        "    #\n",
        "    # `batch` contains three pytorch tensors:\n",
        "    #   [0]: input ids \n",
        "    #   [1]: attention masks\n",
        "    #   [2]: labels \n",
        "    input_ids, attention_mask, labels = batch\n",
        "    input_ids = torch.IntTensor(input_ids).to(device)\n",
        "    attention_mask = torch.IntTensor(attention_mask).to(device)\n",
        "    labels = torch.LongTensor(labels).to(device)\n",
        "\n",
        "    output = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "    # output values prior to applying an activation function like the \n",
        "    # softmax.\n",
        "    loss, logits = output[:2]\n",
        "    \n",
        "    list_hyp = []\n",
        "    list_label = []\n",
        "    hyp = torch.topk(logits, k=1, dim=-1)[1]\n",
        "    for j in range(len(hyp)):\n",
        "        list_hyp.append(i2w[hyp[j].item()])\n",
        "        list_label.append(i2w[labels[j][0].item()])\n",
        "    \n",
        "    return loss, list_label, list_hyp\n",
        "\n",
        "model_result = train(model, train_loader, dev_loader, optimizer, device=\"cuda\", epochs=2)\n",
        "model_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "S5Y39LHXgYkx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5Y39LHXgYkx",
        "outputId": "28c5b5c0-cfa7-4ea6-d3b0-b1af5f1eeb5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [00:11<00:00, 15.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      index label\n",
            "0         0    no\n",
            "1         1    no\n",
            "2         2    no\n",
            "3         3    no\n",
            "4         4    no\n",
            "...     ...   ...\n",
            "2795   2795    no\n",
            "2796   2796    no\n",
            "2797   2797   yes\n",
            "2798   2798    no\n",
            "2799   2799   yes\n",
            "\n",
            "[2800 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Evaluate on test\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "# Tracking variables \n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "\n",
        "pbar = tqdm(test_loader)\n",
        "for batch in pbar:\n",
        "    loss, y_true, y_pred = forward_sequence_classification(model, batch[:-1], i2w=i2w, device=\"cuda\")\n",
        "    total_loss += loss.item()\n",
        "    list_hyp.extend(y_pred)\n",
        "    list_label.extend(y_true)\n",
        "\n",
        "#Save Prediction\n",
        "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
        "df.to_csv('result.txt', index=False)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eBCK63jyhlQu",
      "metadata": {
        "id": "eBCK63jyhlQu"
      },
      "source": [
        "Test fine-tuned model on sample sentences using Test Sample"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get metrics for test data\n",
        "test_metrics = text_classification_metrics_fn(list_hyp, list_label)\n",
        "\n",
        "print(f'Test Loss: {total_loss/len(test_loader):.3f} | Test Acc: {test_metrics[\"accuracy\"]*100:.2f}% | Test F1: {test_metrics[\"f1\"]*100:.2f}% | Test Precision: {test_metrics[\"precision\"]*100:.2f}% | Test Recall: {test_metrics[\"recall\"]*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhGvclxgQOvJ",
        "outputId": "e6e640ee-e25f-4b0d-c135-7cfe3400c0f9"
      },
      "id": "fhGvclxgQOvJ",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.295 | Test Acc: 89.04% | Test F1: 85.58% | Test Precision: 85.34% | Test Recall: 85.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "guYb6r2rg9PC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYb6r2rg9PC",
        "outputId": "c11b98ee-c3e2-42f8-a0be-09788bf277bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: jek dajal ga depok bang | Label : no (99.918%)\n"
          ]
        }
      ],
      "source": [
        "text = test_dataset.__getitem__(0)[2]\n",
        "subwords = tokenizer.encode(text)\n",
        "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "logits = model(subwords)[0]\n",
        "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
        "\n",
        "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "52d90d3cc821dd0beedd6e719dbdecc722c226b9d90ed1b663c34e1877f1142e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}